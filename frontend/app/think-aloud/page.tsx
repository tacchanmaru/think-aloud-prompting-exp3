'use client';

import { useState, useRef, useEffect, useCallback, Suspense } from 'react';
import { useRouter, useSearchParams } from 'next/navigation';
import ProductImageUploadPhase from '../components/ProductImageUploadPhase';
import { useTimer } from '../contexts/TimerContext';
import { useAuth } from '../contexts/AuthContext';
import { saveExperimentData } from '../../lib/experimentService';
import { ThinkAloudExperimentResult } from '../../lib/types';
import { ExperimentPageType } from '../../lib/experimentUtils';


// =========== ThinkAloudPage Component ===========
function ThinkAloudPage() {
    const router = useRouter();
    const searchParams = useSearchParams();
    const isPractice = searchParams.get('practice') === 'true';
    const { startTimer, stopTimer, getStartTimeISO, getEndTimeISO, getDurationSeconds } = useTimer();
    const { userId } = useAuth();
    
    const [mode, setMode] = useState<'upload' | 'edit'>('upload');
    
    // Application state
    const [imagePreview, setImagePreview] = useState<string | null>(null);
    
    // Text editing state
    const [textContent, setTextContent] = useState('');
    const [modificationHistory, setModificationHistory] = useState<{
        utterance: string;
        editPlan: string;
        originalText: string;
        modifiedText: string;
        pastUtterances: string;
        historySummary: string;
    }[]>([]);
    const [historySummary, setHistorySummary] = useState('');
    const [originalText, setOriginalText] = useState('');
    
    // Audio recording state
    const [isRecording, setIsRecording] = useState(false);
    const [isTranscribing, setIsTranscribing] = useState(false);
    const [error, setError] = useState<string | null>(null);
    const [transcriptItems, setTranscriptItems] = useState<{id: number, text: string, utteranceText: string, isProcessed: boolean}[]>([]);
    
    // Utterance buffering state (matching archive backend logic)
    const [utteranceBuffer, setUtteranceBuffer] = useState<string[]>([]);
    const [isProcessing, setIsProcessing] = useState(false);
    const lastCompleteTimeRef = useRef<number>(Date.now());
    const [isDescriptionClicked, setIsDescriptionClicked] = useState(false);
    const [pastUtterances, setPastUtterances] = useState<string>('');  // éå»ã®ç™ºè©±ã‚’ã€Œã€ã€ã§åŒºåˆ‡ã£ã¦ä¿å­˜
    
    const websocketRef = useRef<WebSocket | null>(null);
    const audioContextRef = useRef<AudioContext | null>(null);
    const audioWorkletNodeRef = useRef<AudioWorkletNode | null>(null);
    const isConnectedRef = useRef<boolean>(false);
    const isRecordingStateRef = useRef<boolean>(false);
    const streamRef = useRef<MediaStream | null>(null);
    const descriptionDisplayRef = useRef<HTMLDivElement | null>(null);
    const isWaitingPermissionRef = useRef<boolean>(false);

    // Buffer processing logic (matching archive backend) - moved inline to processBufferedUtterances

    const processTextModification = useCallback(async (utterance: string) => {
        try {
            console.log('Processing text modification for utterance:', utterance);
            console.log('Current text:', textContent);
            
            const response = await fetch('/api/text-modification', {
                method: 'POST',
                headers: {
                    'Content-Type': 'application/json',
                },
                body: JSON.stringify({
                    text: textContent,
                    utterance: utterance,
                    pastUtterances: pastUtterances,
                    imageBase64: imagePreview ? imagePreview.split(',')[1] : undefined,
                    history: modificationHistory,
                    historySummary: historySummary
                }),
            });

            if (!response.ok) {
                throw new Error(`API error: ${response.status}`);
            }

            const result = await response.json();
            
            if (result.error) {
                throw new Error(result.error);
            }

            if (result.shouldEdit && result.modifiedText) {
                console.log('Text modification applied:', result.plan);
                
                // Update text content
                const previousText = textContent;
                setTextContent(result.modifiedText);
                
                // Add to modification history
                const newHistoryItem = {
                    utterance: utterance,
                    editPlan: result.plan || '',
                    originalText: previousText,
                    modifiedText: result.modifiedText,
                    pastUtterances: pastUtterances,
                    historySummary: historySummary,
                };
                
                const updatedHistory = [...modificationHistory, newHistoryItem];
                setModificationHistory(updatedHistory);
                
                // Update history summary asynchronously
                updateHistorySummary(updatedHistory);
                
                console.log('Text successfully modified');
            } else {
                console.log('No modification needed for utterance:', utterance);
            }
            
        } catch (error) {
            console.error('Error in text modification:', error);
            throw error; // Re-throw to be handled by caller
        }
    }, [textContent, imagePreview, modificationHistory, historySummary, pastUtterances]);

    const updateHistorySummary = useCallback(async (history: typeof modificationHistory) => {
        // history summaryã®æ›´æ–°ã¯ç·¨é›†å±¥æ­´ãŒ2ã¤ä»¥ä¸Šã®å ´åˆã®ã¿å®Ÿè¡Œ
        if (history.length < 2) {
            return;
        }

        try {
            console.log('Updating history summary for', history.length, 'modifications');
            
            const response = await fetch('/api/history-summary', {
                method: 'POST',
                headers: {
                    'Content-Type': 'application/json',
                },
                body: JSON.stringify({
                    history: history.map(item => ({
                        utterance: item.utterance,
                        editPlan: item.editPlan,
                        originalText: item.originalText,
                        modifiedText: item.modifiedText,
                    })),
                    currentSummary: historySummary
                }),
            });

            if (!response.ok) {
                throw new Error(`History summary API error: ${response.status}`);
            }

            const result = await response.json();
            
            if (result.historySummary) {
                setHistorySummary(result.historySummary);
                console.log('History summary updated:', result.historySummary);
            }
            
        } catch (error) {
            console.error('Error updating history summary:', error);
            // History summaryæ›´æ–°ã®å¤±æ•—ã¯è‡´å‘½çš„ã‚¨ãƒ©ãƒ¼ã§ã¯ãªã„ãŸã‚ã€ãƒ¡ã‚¤ãƒ³å‡¦ç†ã¯ç¶™ç¶š
        }
    }, []);

    const processBufferedUtterances = useCallback(async () => {
        if (isProcessing) return;
        
        // Check buffer conditions inline
        let shouldProcess = false;
        if (utteranceBuffer.length >= 3) {
            console.log(`Buffer full, processing ${utteranceBuffer.length} utterances`);
            shouldProcess = true;
        } else if (utteranceBuffer.length > 0) {
            const currentTime = Date.now();
            const timeSinceLastComplete = (currentTime - lastCompleteTimeRef.current) / 1000;
            if (timeSinceLastComplete >= 4.0) {
                console.log(`Timeout processing after ${timeSinceLastComplete.toFixed(1)} seconds`);
                shouldProcess = true;
            }
        }
        
        if (!shouldProcess || utteranceBuffer.length === 0) return;
        
        setIsProcessing(true);
        
        try {
            // Get all utterances and clear buffer
            const utterancesToProcess = [...utteranceBuffer];
            setUtteranceBuffer([]);
            
            // Combine utterances (matching archive backend)
            const combinedUtterance = utterancesToProcess.join('');
            
            console.log('Processing buffered utterances:', utterancesToProcess);
            
            await processTextModification(combinedUtterance);
            
            // Add current utterance to past utterances
            setPastUtterances(prev => {
                if (prev) {
                    return prev + 'ã€' + combinedUtterance;
                } else {
                    return combinedUtterance;
                }
            });
            
            // Clear only the transcript items that were processed
            setTranscriptItems(prev => prev.filter(item => 
                !utterancesToProcess.includes(item.utteranceText)
            ));
            
        } catch (error) {
            console.error('Error processing buffered utterances:', error);
            setError(`ãƒ†ã‚­ã‚¹ãƒˆä¿®æ­£ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: ${error instanceof Error ? error.message : String(error)}`);
        } finally {
            setIsProcessing(false);
        }
    }, [utteranceBuffer, isProcessing, processTextModification]);

    // Periodic buffer check (matching archive backend - 0.1 second intervals)
    useEffect(() => {
        const intervalId = setInterval(() => {
            processBufferedUtterances();
        }, 100); // 0.1 seconds

        return () => clearInterval(intervalId);
    }, [processBufferedUtterances]); // Dependencies for useEffect

    // å‰ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—ã™ã‚‹é–¢æ•°
    const getPreviousText = () => {
        if (modificationHistory.length > 0) {
            if (modificationHistory.length === 1) return originalText;
            if (modificationHistory.length >= 2) return modificationHistory[modificationHistory.length - 2].modifiedText;
            return originalText;
        }
        return originalText;
    };

    // è¡Œå˜ä½ã§ã®å·®åˆ†ã‚’è¨ˆç®—ã™ã‚‹é–¢æ•°
    const calculateLineDiff = (originalText: string, currentText: string) => {
        const originalLines = originalText.split('\n');
        const currentLines = currentText.split('\n');
        
        // LCSï¼ˆæœ€é•·å…±é€šéƒ¨åˆ†åˆ—ï¼‰ã‚’ä½¿ç”¨ã—ãŸå·®åˆ†è¨ˆç®—
        const lcs = calculateLCS(originalLines, currentLines);
        const result: Array<{ content: string; type: 'unchanged' | 'added' | 'removed' }> = [];
        
        let i = 0, j = 0, k = 0;
        
        while (i < originalLines.length || j < currentLines.length) {
            if (k < lcs.length && i < originalLines.length && j < currentLines.length && 
                originalLines[i] === lcs[k] && currentLines[j] === lcs[k]) {
                // å…±é€šã®è¡Œ
                result.push({ content: currentLines[j], type: 'unchanged' });
                i++;
                j++;
                k++;
            } else if (i < originalLines.length && (k >= lcs.length || originalLines[i] !== lcs[k])) {
                // å‰Šé™¤ã•ã‚ŒãŸè¡Œ
                result.push({ content: originalLines[i], type: 'removed' });
                i++;
            } else if (j < currentLines.length && (k >= lcs.length || currentLines[j] !== lcs[k])) {
                // è¿½åŠ ã•ã‚ŒãŸè¡Œ
                result.push({ content: currentLines[j], type: 'added' });
                j++;
            }
        }
        
        return result;
    };

    // æœ€é•·å…±é€šéƒ¨åˆ†åˆ—ï¼ˆLCSï¼‰ã‚’è¨ˆç®—ã™ã‚‹é–¢æ•°
    const calculateLCS = (arr1: string[], arr2: string[]): string[] => {
        const m = arr1.length;
        const n = arr2.length;
        const dp: number[][] = Array(m + 1).fill(null).map(() => Array(n + 1).fill(0));
        
        // DPãƒ†ãƒ¼ãƒ–ãƒ«ã‚’æ§‹ç¯‰
        for (let i = 1; i <= m; i++) {
            for (let j = 1; j <= n; j++) {
                if (arr1[i - 1] === arr2[j - 1]) {
                    dp[i][j] = dp[i - 1][j - 1] + 1;
                } else {
                    dp[i][j] = Math.max(dp[i - 1][j], dp[i][j - 1]);
                }
            }
        }
        
        // LCSã‚’å¾©å…ƒ
        const lcs: string[] = [];
        let i = m, j = n;
        
        while (i > 0 && j > 0) {
            if (arr1[i - 1] === arr2[j - 1]) {
                lcs.unshift(arr1[i - 1]);
                i--;
                j--;
            } else if (dp[i - 1][j] > dp[i][j - 1]) {
                i--;
            } else {
                j--;
            }
        }
        
        return lcs;
    };

    const handleUploadComplete = async (imageFile: File, imagePreview: string, generatedText: string) => {
        setImagePreview(imagePreview);
        setTextContent(generatedText);
        setOriginalText(generatedText);
        
        // Start recording and wait for it to be fully connected
        try {
            await startRecordingAndWaitForConnection();
            
            // Once recording is active, start timer and switch to edit mode
            startTimer();
            setMode('edit');
        } catch (error) {
            console.error('Failed to start recording:', error);
            setError('éŒ²éŸ³ã®é–‹å§‹ã«å¤±æ•—ã—ã¾ã—ãŸã€‚ãƒšãƒ¼ã‚¸ã‚’æ›´æ–°ã—ã¦ã‚‚ã†ä¸€åº¦ãŠè©¦ã—ãã ã•ã„ã€‚');
        }
    };

    const startRecordingAndWaitForConnection = async () => {
        try {
            // Request microphone permission first
            const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
            stream.getTracks().forEach(track => track.stop());
            
            // Start the recording process and wait for connection
            await startRecording();
            
            // Add a simple delay to ensure WebSocket connection is established
            // Since startRecording already handles the WebSocket connection process
            await new Promise(resolve => setTimeout(resolve, 1500));
            
        } catch (error) {
            console.error('Microphone permission denied:', error);
            setError('ãƒã‚¤ã‚¯ã®è¨±å¯ãŒå¿…è¦ã§ã™ã€‚ãƒ–ãƒ©ã‚¦ã‚¶ã®è¨­å®šã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚');
            throw error;
        }
    };

    const startRecording = async () => {
        setError(null);
        if (isRecording) return;

        try {
            setIsTranscribing(true);
            isConnectedRef.current = false;

            // Get ephemeral token for direct WebSocket connection
            const tokenResponse = await fetch('/api/auth/openai-token', { method: 'POST' });
            if (!tokenResponse.ok) {
                const errorBody = await tokenResponse.text();
                throw new Error(`Ephemeral tokenã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸã€‚ Status: ${tokenResponse.status}, Body: ${errorBody}`);
            }
            const responseData = await tokenResponse.json();
            const ephemeralToken = responseData.token;

            if (typeof ephemeralToken !== 'string') {
                console.error("ç„¡åŠ¹ãªephemeral tokenã‚’å—ä¿¡ã—ã¾ã—ãŸ:", responseData);
                throw new Error('Ephemeral tokenãŒæ–‡å­—åˆ—ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚');
            }

            // Setup direct WebSocket connection with subprotocols
            const ws = new WebSocket(
                'wss://api.openai.com/v1/realtime?intent=transcription',
                [
                    'realtime',
                    `openai-insecure-api-key.${ephemeralToken}`,
                    'openai-beta.realtime-v1',
                ]
            );
            websocketRef.current = ws;

            ws.onopen = async () => {
                isConnectedRef.current = true;
                isRecordingStateRef.current = true;
                setIsRecording(true);
                setIsTranscribing(false);
                
                // Start timer when audio recording actually begins
                if (isWaitingPermissionRef.current) {
                    startTimer();
                    setMode('edit');
                    isWaitingPermissionRef.current = false;
                }

                // Send transcription session configuration
                const configMessage = {
                    type: 'transcription_session.update',
                    session: {
                        input_audio_transcription: {
                            model: 'gpt-4o-transcribe',
                            language: 'ja',
                        },
                        input_audio_noise_reduction: { type: 'near_field' },
                        turn_detection: {
                            type: 'server_vad',
                        },
                    },
                };
                ws.send(JSON.stringify(configMessage));

                // Setup audio processing
                try {
                    const stream = await navigator.mediaDevices.getUserMedia({
                        audio: { sampleRate: 24000, channelCount: 1 }
                    });
                    streamRef.current = stream;

                    // Create audio worklet processor code inline
                    const audioWorkletCode = `
                        class PCMProcessor extends AudioWorkletProcessor {
                            constructor() {
                                super();
                                this.sampleRate = 24000;
                                this.chunkSize = this.sampleRate * 0.1;
                                this.buffer = [];
                            }

                            process(inputs, outputs, parameters) {
                                const input = inputs[0];
                                if (input && input[0]) {
                                    const float32Data = input[0];
                                    this.buffer.push(...float32Data);

                                    while (this.buffer.length >= this.chunkSize) {
                                        const chunk = this.buffer.slice(0, this.chunkSize);
                                        this.buffer = this.buffer.slice(this.chunkSize);

                                        const int16Buffer = new Int16Array(chunk.length);
                                        for (let i = 0; i < chunk.length; i++) {
                                            int16Buffer[i] = Math.max(-1, Math.min(1, chunk[i])) * 0x7fff;
                                        }

                                        this.port.postMessage(int16Buffer.buffer, [int16Buffer.buffer]);
                                    }
                                }
                                return true;
                            }
                        }
                        registerProcessor('pcm-processor', PCMProcessor);
                    `;

                    const audioContext = new AudioContext({ sampleRate: 24000 });
                    audioContextRef.current = audioContext;

                    if (audioContext.state === 'suspended') {
                        await audioContext.resume();
                    }

                    const blob = new Blob([audioWorkletCode], { type: 'application/javascript' });
                    const workletURL = URL.createObjectURL(blob);
                    await audioContext.audioWorklet.addModule(workletURL);
                    URL.revokeObjectURL(workletURL);

                    const source = audioContext.createMediaStreamSource(stream);
                    const pcmProcessor = new AudioWorkletNode(audioContext, 'pcm-processor');
                    audioWorkletNodeRef.current = pcmProcessor;

                    pcmProcessor.port.onmessage = (event) => {
                        if (ws.readyState === WebSocket.OPEN && event.data) {
                            try {
                                const buffer = Buffer.from(event.data);
                                const base64Audio = buffer.toString('base64');

                                ws.send(JSON.stringify({
                                    type: 'input_audio_buffer.append',
                                    audio: base64Audio,
                                }));
                            } catch (audioError) {
                                console.warn('Failed to send audio data:', audioError);
                            }
                        }
                    };

                    source.connect(pcmProcessor);
                    pcmProcessor.connect(audioContext.destination);

                } catch (audioErr) {
                    console.error("Error accessing microphone:", audioErr);
                    setError(`ãƒã‚¤ã‚¯ã‚¢ã‚¯ã‚»ã‚¹ã‚¨ãƒ©ãƒ¼: ${audioErr instanceof Error ? audioErr.message : String(audioErr)}`);
                }
            };

            ws.onmessage = (event) => {
                if (!isConnectedRef.current || !isRecordingStateRef.current) {
                    return;
                }

                const message = JSON.parse(event.data);

                switch (message.type) {
                    case 'conversation.item.input_audio_transcription.completed':
                        if (isConnectedRef.current && isRecordingStateRef.current) {
                            if (message.transcript) {
                                console.log('Transcript received:', message.transcript);
                                // Add to utterance buffer (only if not empty)
                                const utterance = message.transcript.trim();
                                if (utterance) {
                                    // Add new transcript item for display
                                    const newItem = {
                                        id: Date.now(),
                                        text: message.transcript,
                                        utteranceText: utterance,
                                        isProcessed: false
                                    };
                                    setTranscriptItems(prev => [...prev, newItem]);
                                    
                                    setUtteranceBuffer(prev => [...prev, utterance]);
                                    lastCompleteTimeRef.current = Date.now();
                                    console.log(`Added utterance to buffer: "${utterance}"`);
                                }
                            }
                        }
                        break;
                    case 'error':
                        console.error('Server error:', message);
                        setError(`OpenAI API ã‚¨ãƒ©ãƒ¼: ${message.error?.message || 'Unknown error'}`);
                        break;
                }
            };

            ws.onerror = (error) => {
                console.error('WebSocket error:', error);
                setError(`WebSocketæ¥ç¶šã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚`);
                stopRecording();
            };

            ws.onclose = (event) => {
                if (isConnectedRef.current && isRecording) {
                    setError(`WebSocketæ¥ç¶šãŒäºˆæœŸã›ãšé–‰ã˜ã‚‰ã‚Œã¾ã—ãŸ: ${event.reason || 'Unknown reason'}`);
                    stopRecording();
                }
            };

        } catch (error) {
            const message = error instanceof Error ? error.message : "äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚";
            setError(`éŒ²éŸ³é–‹å§‹ã«å¤±æ•—ã—ã¾ã—ãŸ: ${message}`);
            stopRecording();
        }
    };

    const stopRecording = () => {
        isConnectedRef.current = false;
        isRecordingStateRef.current = false;
        setIsRecording(false);
        setIsTranscribing(false);

        if (websocketRef.current && websocketRef.current.readyState === WebSocket.OPEN) {
            try {
                websocketRef.current.send(JSON.stringify({
                    type: 'input_audio_buffer.clear'
                }));
                setTimeout(() => {
                    if (websocketRef.current) {
                        websocketRef.current.close();
                        websocketRef.current = null;
                    }
                }, 100);
            } catch (error) {
                console.warn('Failed to clear audio buffer:', error);
                if (websocketRef.current) {
                    websocketRef.current.close();
                    websocketRef.current = null;
                }
            }
        } else if (websocketRef.current) {
            websocketRef.current.close();
            websocketRef.current = null;
        }

        if (audioWorkletNodeRef.current) {
            audioWorkletNodeRef.current.disconnect();
            audioWorkletNodeRef.current = null;
        }

        if (audioContextRef.current && audioContextRef.current.state !== 'closed') {
            audioContextRef.current.close();
            audioContextRef.current = null;
        }

        if (streamRef.current) {
            streamRef.current.getTracks().forEach(track => track.stop());
            streamRef.current = null;
        }
    };

    const handleComplete = async () => {
        try {
            // ã‚¿ã‚¤ãƒãƒ¼ã‚’åœæ­¢
            stopTimer();
            
            // å®Ÿé¨“ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™
            const experimentData: ThinkAloudExperimentResult = {
                userId: userId || 0, // 1-100ã®ç¯„å›²ã®userId
                experimentType: 'think-aloud',
                productId: 'product1', // ç¾åœ¨ã¯product1å›ºå®š
                originalText,
                finalText: textContent,
                startTime: getStartTimeISO() || new Date().toISOString(),
                endTime: getEndTimeISO(),
                durationSeconds: getDurationSeconds(),
                intermediateSteps: modificationHistory.map(item => ({
                    utterance: item.utterance,
                    past_utterances: item.pastUtterances,
                    edit_plan: item.editPlan,
                    modified_text: item.modifiedText,
                    history_summary: item.historySummary
                })),
                isPracticeMode: isPractice,
            };

            // ä¿å­˜ã‚’è©¦è¡Œ
            const saveSuccess = await saveExperimentData(experimentData);
            
            if (saveSuccess) {
                alert('å®Ÿé¨“ãŒå®Œäº†ã—ã¾ã—ãŸã€‚ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã—ãŸã€‚');
            } else {
                alert('å®Ÿé¨“ã¯å®Œäº†ã—ã¾ã—ãŸãŒã€ãƒ‡ãƒ¼ã‚¿ã®ä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸã€‚ç®¡ç†è€…ã«ãŠçŸ¥ã‚‰ã›ãã ã•ã„ã€‚');
            }
            
        } catch (error) {
            console.error('Complete error:', error);
            alert('å®Ÿé¨“ã¯å®Œäº†ã—ã¾ã—ãŸãŒã€ãƒ‡ãƒ¼ã‚¿ã®ä¿å­˜ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚ç®¡ç†è€…ã«ãŠçŸ¥ã‚‰ã›ãã ã•ã„ã€‚');
        } finally {
            router.push('/');
        }
    };

    return (
        <div className="app-container">
            {mode === 'upload' ? (
                <ProductImageUploadPhase onComplete={handleUploadComplete} isPractice={isPractice} pageType={ExperimentPageType.ThinkAloud} />
            ) : (
                <div className="product-layout">
                    <div className="product-image-container">
                        {imagePreview && (
                            <img src={imagePreview} alt="å•†å“ç”»åƒ" className="product-image" />
                        )}
                    </div>
                    <div className="product-description-container">
                        <div className="text-header">
                            <h3 className="product-description-header">å•†å“èª¬æ˜ï¼ˆã‚¿ãƒƒãƒ—ã§å‰Šé™¤è¡Œã‚‚è¡¨ç¤ºï¼‰</h3>
                        </div>
                        <div
                            ref={descriptionDisplayRef}
                            className="text-editor cursor-pointer select-none"
                            onTouchStart={() => setIsDescriptionClicked(true)}
                            onTouchEnd={() => setIsDescriptionClicked(false)}
                            style={{ 
                                minHeight: 'calc(12px * 1.6 * 5)', // manual-editã¨åŒã˜æœ€å°5è¡Œã®é«˜ã•
                                whiteSpace: 'pre-line', 
                                wordWrap: 'break-word' 
                            }}
                        >
                            {textContent ? (
                                modificationHistory.length > 0 ? (
                                    <div>
                                        {calculateLineDiff(getPreviousText() || '', textContent)
                                            .filter(line => isDescriptionClicked || line.type !== 'removed')
                                            .map((line, index) => (
                                            <div
                                                key={index}
                                                className={`${
                                                    line.type === 'added'
                                                        ? 'bg-yellow-100'
                                                        : line.type === 'removed'
                                                        ? 'bg-red-100'
                                                        : 'bg-white'
                                                } ${line.content.trim() === '' ? 'min-h-[1em]' : ''}`}
                                            >
                                                {line.content || '\u00A0'}
                                            </div>
                                        ))}
                                    </div>
                                ) : (
                                    <div>{textContent}</div>
                                )
                            ) : (
                                <span style={{ color: '#888' }}>å•†å“èª¬æ˜ã‚’ç·¨é›†ã—ã¦ãã ã•ã„...</span>
                            )}
                        </div>
                            <div className="controls">
                                <div className="transcription-display">
                                    <div className="transcription-header">
                                        {isProcessing ? 'âš™ï¸ ãƒ†ã‚­ã‚¹ãƒˆä¿®æ­£ä¸­...' : 'ğŸ™ï¸ éŸ³å£°èªè­˜ä¸­'}
                                    </div>
                                    <div className="transcript-items">
                                        {transcriptItems.length === 0 ? (
                                            <span className="no-transcript">
                                                ã¾ã éŸ³å£°ãŒèªè­˜ã•ã‚Œã¦ã„ã¾ã›ã‚“
                                            </span>
                                        ) : (
                                            <span className="transcript-text">
                                                {transcriptItems.map((item) => item.text).join('')}
                                            </span>
                                        )}
                                    </div>
                                </div>
                                <button
                                    className="complete-button-full"
                                    onClick={handleComplete}
                                    disabled={isProcessing || modificationHistory.length === 0}
                                >
                                    ç·¨é›†å®Œäº†
                                </button>
                            </div>
                        </div>
                    </div>
                )}

            {error && <div className="error">{error}</div>}
        </div>
    );
}

function ThinkAloudPageWithSuspense() {
    return (
        <Suspense fallback={<div>Loading...</div>}>
            <ThinkAloudPage />
        </Suspense>
    );
}

export default ThinkAloudPageWithSuspense;